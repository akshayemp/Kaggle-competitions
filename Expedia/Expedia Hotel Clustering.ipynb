{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expedia Hotel Recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge we need to predict what hotel a user will book based on attributes that user searched on Expedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the prediction problem let's first understand the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expedia - Contextualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the expedia website we get following information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text box labelled *Going To* maps to *srch_destination_type_id*, *hotel_continent*, *hotel_country*,*hotel_market* fields in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text box labelled *Check-in* maps to the *srch_ci* field in the data, and the box labelled *Check out* maps to *srch_co* field in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box labelled *Guests* maps to *srch_adults_cnt* , *srch_children_cnt*, and *srch_rm_cnt* fields in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text box labelled *Add a Flight* maps to the *is_package* field in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*site_name* is the name of the site you visited, it can be main *Expedia* or any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "destinations = pd.read_csv(\"../../Kaggle Data/Expedia/Data/destinations.csv\")\n",
    "test = pd.read_csv(\"../../Kaggle Data/Expedia/Data/test.csv\")\n",
    "train = pd.read_csv(\"../../Kaggle Data/Expedia/Data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 37 million training set rows, and 2 million testing set rows, which will make this problem a bit challenging to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some analysis on the data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * *data_time* is very useful and needs conversion.\n",
    " * Most of the columns are integers or floats, so we can't do any feature engineering because we don't know exactly which each value means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some analysis on the test data:\n",
    "\n",
    "* dates in test.csv are later than dates in train.csv.\n",
    "* user ids in test.csv are a subset of the user ids in train.csv, given the overlapping integer ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we're predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll be predicting which *hotel_cluster* a user will book after a given search. There are 100 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scoring metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring metric is Mean Average Precision on 5 cluster predictions each row and will be scored on whether or not the correct prediction appears in our list. If the correct prediction comes earlier in the list, we get more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if the \"correct\" cluster is 3, and we predict 4,43,60,3,20 our score will be lower than if we predict 3,4,43,60,20. So, we should put predictions which we're more certain about earlier in our list of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring hotel clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"hotel_cluster\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"hotel_cluster\"].value_counts().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of hotels in each cluster is evenly distributed. There doesn't appear to be any relationship between cluster number and number of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring train and test user ids**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesized that all the test user ids in test dataframe can be found in the train dataframe. We can do this by finding the unique values for user_id in test, and seeing if they all exist in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids = set(test.user_id.unique())\n",
    "train_ids = set(train.user_id.unique())\n",
    "intersection_count = len(test_ids & train_ids)\n",
    "intersection_count == len(test_ids)a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeay!! So we were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling our Kaggle data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so 37 million rows in our training set which makes it hard to experiment with different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"date_time\"] = pd.to_datetime(train[\"date_time\"])\n",
    "train[\"year\"] = train[\"date_time\"].dt.year\n",
    "train[\"month\"] = train[\"date_time\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick 10,000 users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "unique_users = train.user_id.unique()\n",
    "\n",
    "sel_user_ids = [unique_users[i] for i in sorted(\n",
    "        random.sample(range(len(unique_users)),10000))]\n",
    "sel_train = train[train.user_id.isin(sel_user_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a DataFrame called sel_train that only contains data from 10000 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick new training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = sel_train[(sel_train.year == 2013) | ((sel_train.year == 2014) \n",
    "                                           & (sel_train.month < 8))]\n",
    "t2 = sel_train[((sel_train.year == 2014) & (sel_train.month >=8))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we picked up new training and testing sets from *sel_train*. Which are t1 and t2 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original train and test dataframes, test contained data from 2015, and train contained data from 2013 and 2014. We split the data so that anything after *July 2014* is in *t2*, and anything before is in *t1*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove click events**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If is_booking is 0, it represents a click, and a 1 represents a booking. We need to sample our t2 such that it contains only booking as did our original test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2 = t2[t2.is_booking == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one of the simplest things we can do is that we could try on this data to find the most common clusters, then use them as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_common_clusters = list(train.hotel_cluster.value_counts().head().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generatig predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn *most_common_clusters* into a list of predictions by making same predictions for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the data leak in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = [most_common_clusters for i in range(t2.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a list with as many elements as there are rows in t2. Each element will be equal to most_common_clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluting error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evalute error, we'll need to figure out how to compute Mean Average Precision. Ben Hamner has written an implementation. *ml_metrics* package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this package and compute our error metric with the mapk method in ml_metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ml_metrics as metrics\n",
    "target = [[l] for l in t2[\"hotel_cluster\"]]\n",
    "metrics.mapk(target,predictions, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target needs to be in list of lists format for mapk to work, so we convert the *hotel_cluster* column of t2 into a list of lists. Then, we call use mapk method with our target, our predictions and number of predictions we want to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "dest_small = pca.fit_transform(\n",
    "    destinations[[\"d{0}\".format(i+1) for i in range(149)]])\n",
    "dest_small = pd.DataFrame(dest_small)\n",
    "dest_small[\"srch_destination_id\"] = destinations[\"srch_destination_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code compresses 149 columns in the destinations down to 3 columns, and creates a new DataFrame call dest_small. We preserve most of the variance in destinations while doing this, so we don't lose a lot of information, but save a lot of runtime for a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our data to some level. We will generate and clean our features.\n",
    "\n",
    "* Generate new date features based on date_time, srch_ci and srch_co.\n",
    "* Remove non-numeric columns like date_time.\n",
    "* Add in features from dest_small.\n",
    "* Replace any missing values with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_fast_features(df):\n",
    "    df[\"data_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"srch_ci\"] = pd.to_datetime(df[\"srch_ci\"], format='%Y-%m-%d',errors='coerce')\n",
    "    df['srch_co'] = pd.to_datetime(df['srch_co'], format='%Y-%m-%d',errors='coerce')\n",
    "    \n",
    "    props = {}\n",
    "    for prop in [\"month\",\"day\",\"hour\",\"minute\", \"dayofweek\",\"quater\"]:\n",
    "        props[prop] = getattr(df[\"date_time\"].dt, prop)\n",
    "        \n",
    "    carryover = [p for p in df.columns if p not in [\"date_time\",\"srch_ci\",\"srch_co\"]]\n",
    "    for prop in carryover:\n",
    "        props[prop] = df[prop]\n",
    "    \n",
    "    date_prop = [\"month\",\"day\",\"dayofweek\",\"quater\"]\n",
    "    for prop in date_props:\n",
    "        props[\"ci_{0}\".format(prop)] = getattr(df[\"srch_ci\"].dt,prop)\n",
    "        props[\"co_{0}\".format(prop)] = getattr(df[\"srch_co\"].dt,prop)\n",
    "    props[\"stay_span\"] = (df[\"srch_co\"] - df[\"srch_ci\"]).astype('timedelta64[h]')\n",
    "    \n",
    "    ret = pd.DataFrame(props)\n",
    "    \n",
    "    ret = ret.join(dest_small, on=\"srch_destination_id\",how='left', rsuffix = 'dest')\n",
    "    ret = ret.drop(\"srch_destination_iddest\",axis=1)\n",
    "    return ret\n",
    "\n",
    "df = calc_fast_features(t1)\n",
    "df.fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply 3-fold cross validation across the training set to generate a reliable error estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [c for cc in df.columns if c not in [\"hotel_cluster\"]]\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, min_weight_fraction_leaf=0.1)\n",
    "scores = cross_validation.cross_val_score(clf,df[predictors]\n",
    "                                          ,df['hotel_cluster'],cv=3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using 3-fold CV we can clearly see that we don't get a good accuracy and machine learning is not a good approach to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train random forests, but for each forest will predict only a single hotel cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from itertools import chain\n",
    "\n",
    "all_probs = []\n",
    "unique_clusters = df[\"hotel_cluster\"].unique()\n",
    "for cluster in unique_clusters:\n",
    "    df['target'] = 1\n",
    "    df[\"target\"]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
